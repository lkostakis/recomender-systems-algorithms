{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file in 0.061 mins\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "leftK_path = '/home/left/github/'\n",
    "\n",
    "filename = 'yelp_academic_dataset_business.json'\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# Store all businesses\n",
    "data = [json.loads(line) for line in \n",
    "        open(leftK_path + filename, 'r', encoding=\"utf8\")]\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Read file in {:.3f} mins\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep businesses which are located in Toronto\n",
    "data = [x for x in data if x['city']=='Toronto']\n",
    "data = np.array(data) # Convert list to numpy array\n",
    "\n",
    "# Create an array with Toronto located businesses with more than 15 reviews\n",
    "business_col = np.array([])\n",
    "for i in range(len(data)):\n",
    "    if data[i]['review_count']>=15:\n",
    "        business_col = np.append(business_col,data[i]['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping function to check if a business is in business_col (Toronto and >=15 reviews)\n",
    "def get_business_index(business_id):\n",
    "    return np.where(business_col == business_id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file in 10.629 mins\n"
     ]
    }
   ],
   "source": [
    "filename = 'yelp_academic_dataset_review.json'\n",
    "start = time.time()\n",
    "\n",
    "user_business = []\n",
    "# Read the json review file line by line and keep the reviews referring to business_col \n",
    "with open(leftK_path + filename,'r',encoding=\"utf8\") as reviews_file:\n",
    "\n",
    "    for line in reviews_file:\n",
    "        line = json.loads(line)\n",
    "        ind = get_business_index(line['business_id'])\n",
    "\n",
    "        if ind.size>0:\n",
    "            user_business.append([line['user_id'],line['business_id'], line['stars'], line['date']])\n",
    "            \n",
    "stop = time.time()\n",
    "print(\"Read file in {:.3f} mins\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User business have all the reviews we want. But using numpy unique will keep the first occurency.\n",
    "# We want to keep last occurency, so a smart way to do so is to read our list with all pairs of \n",
    "# type (UserID, BusinessID, Rating) upside-down(reverse). Then call unique and reconstruct the \n",
    "# total reviews with the given indices.\n",
    "user_business.reverse()\n",
    "from operator import itemgetter\n",
    "user_bus_sorted = sorted(user_business, key=itemgetter(0,1))\n",
    "\n",
    "user_bus_sorted = np.array(user_bus_sorted)\n",
    "pairs = user_bus_sorted[:,0:2]\n",
    "_, idx = np.unique(pairs, axis=0, return_index=True)\n",
    "total_reviews = user_bus_sorted[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dates = total_reviews[:,3]\n",
    "for i in range(len(dates)):\n",
    "    dates[i] = int(datetime.fromisoformat(dates[i]).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse array created in 3.789 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7602, 5677)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the userID column and keep the unique users. Do a for-loop to keep users with >=15 reviews.\n",
    "users = total_reviews[:,0]\n",
    "unique_users, counts = np.unique(users, return_counts=True)\n",
    "uniq_users = []\n",
    "\n",
    "for i in range(len(unique_users)):\n",
    "    if counts[i]>=15:\n",
    "        uniq_users.append(unique_users[i])\n",
    "uniq_users = np.array(uniq_users)\n",
    "\n",
    "# Here keep the unique businesses column\n",
    "businesses = total_reviews[:,1]\n",
    "unique_businesses = np.unique(businesses)\n",
    "\n",
    "start = time.time()\n",
    "# Create the sparse_array using the unique users(rows) and unique businesses (columns)\n",
    "sparse = np.zeros((len(unique_businesses), len(uniq_users)))\n",
    "sparse_dates = np.zeros((len(unique_businesses), len(uniq_users)))\n",
    "for i in range(len(unique_businesses)):\n",
    "    user_index = np.where(unique_businesses[i] == total_reviews)[0]\n",
    "    temp = []\n",
    "    for j in range(len(user_index)):\n",
    "        unique_user_ind = np.where(total_reviews[user_index[j]][0] == uniq_users) \n",
    "        if unique_user_ind[0].size>0:\n",
    "            sparse[i][unique_user_ind[0][0]] = int(float(total_reviews[user_index[j]][2]))\n",
    "            sparse_dates[i][unique_user_ind[0][0]] = int(total_reviews[user_index[j]][3])\n",
    "stop = time.time()\n",
    "print(\"Sparse array created in {:.3f} mins\".format((stop-start)/60))\n",
    "\n",
    "# Sparse array shape indicates the initial unique users and unqiue businesses\n",
    "sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4092, 4794)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prune until nothing to delete\n",
    "\n",
    "stop=0\n",
    "while(stop==0):\n",
    "    # prune the rows(businesses with below 15 reviews)\n",
    "    index_to_delete = []\n",
    "    for i in range(sparse.shape[0]):\n",
    "        nonzero_indexes = np.nonzero(sparse[i])\n",
    "        if len(nonzero_indexes[0])<15:\n",
    "            index_to_delete.append(i)\n",
    "            \n",
    "    #make new sparse array with deleted items and unique_businesses to track ids\n",
    "    unique_businesses = np.delete(unique_businesses, index_to_delete)\n",
    "    sparse = np.delete(sparse, index_to_delete, axis=0)\n",
    "    sparse_dates = np.delete(sparse_dates, index_to_delete, axis=0)\n",
    "    \n",
    "    #prune the columns(users)\n",
    "    transposed_sparse = np.transpose(sparse)\n",
    "    columns_to_delete = []\n",
    "    for i in range(transposed_sparse.shape[0]):\n",
    "        nonzero_values = np.nonzero(transposed_sparse[i])\n",
    "        if len(nonzero_values[0])<15:\n",
    "            columns_to_delete.append(i)\n",
    "            \n",
    "    #make new sparse array with deleted items and uniq_users to track ids\n",
    "    uniq_users = np.delete(uniq_users, columns_to_delete)\n",
    "    sparse = np.delete(sparse, columns_to_delete, axis=1)\n",
    "    sparse_dates = np.delete(sparse_dates, columns_to_delete, axis=1)\n",
    "    \n",
    "    # If nothing to delete -> stop\n",
    "    if len(index_to_delete)==0 and len(columns_to_delete)==0:\n",
    "        stop=1\n",
    "\n",
    "# After pruning we have our sparse array reshaped\n",
    "sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list created in 0.214 mins\n"
     ]
    }
   ],
   "source": [
    "# Create the new data, create a list([userID,businessID,rating]) using our sparse array data\n",
    "data_csv = []\n",
    "start = time.time()\n",
    "for i in range(sparse.shape[1]):\n",
    "    for j in range(sparse.shape[0]):\n",
    "        if sparse[j][i]>0:\n",
    "            data_csv.append([uniq_users[i],unique_businesses[j], sparse[j][i], sparse_dates[j][i]])\n",
    "stop = time.time()\n",
    "print(\"Data list created in {:.3f} mins\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv = sorted(data_csv, key=itemgetter(0,3))\n",
    "len(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = np.array(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       user_id             business_id  rating          date\n",
      "0       --BumyUHiO_7YsHurb9Hkw  vcxvQyAggPqxcHwvJXvjGg     5.0  1.484308e+09\n",
      "1       --BumyUHiO_7YsHurb9Hkw  r_BrIgzYcwo1NAuG9dLbpg     4.0  1.484310e+09\n",
      "2       --BumyUHiO_7YsHurb9Hkw  iZJ5pdY558VodrEumGyVug     5.0  1.484836e+09\n",
      "3       --BumyUHiO_7YsHurb9Hkw  q5xrVJ4kivx_yEfJeOKNYQ     4.0  1.485190e+09\n",
      "4       --BumyUHiO_7YsHurb9Hkw  xsl-d_opm3AU5H2Z-im33g     4.0  1.485190e+09\n",
      "...                        ...                     ...     ...           ...\n",
      "197107  zzmhLxcZ4XZQyz95c_KbOA  ACBFbEW6oa58_RyX9Op-qQ     5.0  1.542595e+09\n",
      "197108  zzmhLxcZ4XZQyz95c_KbOA  lOKgoQtMhnlf6hWvrOiMtQ     2.0  1.546395e+09\n",
      "197109  zzmhLxcZ4XZQyz95c_KbOA  9HWdRtNS0q4_UkEvL14IfA     3.0  1.546396e+09\n",
      "197110  zzmhLxcZ4XZQyz95c_KbOA  bNHeKmkBx5emT9xLfdWyjw     5.0  1.546397e+09\n",
      "197111  zzmhLxcZ4XZQyz95c_KbOA  FyUcIAn8timIFybYpOLbAw     5.0  1.546397e+09\n",
      "\n",
      "[197112 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# We have the file ready from Step 1\n",
    "data_table = pd.DataFrame(data_csv, columns = ['user_id', 'business_id', 'rating', 'date'])\n",
    "print(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table.to_csv(r'data_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data should have 9856 rows \n",
      "\n",
      "9856\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data should have {} rows \\n\".format(math.ceil(len(data_np)*0.05)))\n",
    "sample_data = data_table.sample(frac=0.05)\n",
    "print(len(sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sample ratings to zero\n",
    "for index in sample_data.index:\n",
    "    data_table.at[index, 'rating'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the numpy array, unique_users and unique_business lists\n",
    "data_table_R = data_table.to_numpy()\n",
    "unique_users = np.unique(data_table_R[:,0])\n",
    "unique_business = np.unique(data_table_R[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse array created in 3.214 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "sparse_array = np.zeros((len(unique_users), len(unique_business)))\n",
    "# Create the sparse_array\n",
    "for i in range(len(unique_users)):\n",
    "    # Take all business index that a user has a rating\n",
    "    business_index = np.where( unique_users[i] == data_table['user_id']) \n",
    "    \n",
    "    # For all the businesses indexes find its index in unique_business and assign that index value\n",
    "    # in the sparse array too.\n",
    "    for j in range(len(business_index[0])):\n",
    "        temp_business = data_table['business_id'][business_index[0][j]] #take business_id for given index\n",
    "        unique_business_ind = np.where(temp_business == unique_business)\n",
    "        sparse_array[i][unique_business_ind[0][0]] = data_table['rating'][business_index[0][j]]\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Sparse array created in {:.3f} mins\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPORW APLA NA KSEKINISW APO TO range(1,:) (?) ...\n",
    "j=0\n",
    "embedding = ['']\n",
    "for i in range(data_table_R.shape[0]):\n",
    "    if data_table_R[i][0] == data_table_R[i-1][0]:\n",
    "        embedding[j] += ' ' + data_table_R[i][1]\n",
    "    else:\n",
    "        embedding.append(data_table_R[i][1])\n",
    "        j+=1\n",
    "embedding = embedding[1:]\n",
    "\n",
    "for i in range(len(embedding)):\n",
    "    embedding[i] = embedding[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 100\n",
    "skipgram_model = gensim.models.Word2Vec(embedding, min_count = 1,size = embedding_size, window = 50, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9KBd2YPnRxaNjZz23TUd6A', 0.8016437292098999),\n",
       " ('VJHZxXBd7k9A8cQ4j8Uvkg', 0.7788745164871216),\n",
       " ('Xhf3nW6UahoVpRMkVUlnJA', 0.7710971832275391),\n",
       " ('r395lMbm1ihAQ2sMOcFKow', 0.7645103931427002),\n",
       " ('SYExIal3fh3mS3kpoP9Ixg', 0.7629245519638062),\n",
       " ('SCklYa_jhihWAcfIn-_chg', 0.7504479289054871),\n",
       " ('KpFc2FXm7OXLhoKgKDNz2g', 0.7501605749130249),\n",
       " ('ptmG_KjwGVszU0sPpFGXJQ', 0.7463140487670898),\n",
       " ('gbjIwB_r6y4YhaZMwEK6mw', 0.7453635931015015),\n",
       " ('2QCZyAOB6xXKCdyFPvXXmw', 0.7408798933029175)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model.wv.most_similar('vcxvQyAggPqxcHwvJXvjGg', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_array = sample_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_index(user_id):\n",
    "    ind = np.where(user_id  == unique_users)\n",
    "    return ind[0][0]\n",
    "def find_business(business_id):\n",
    "    ind = np.where(business_id == unique_business)\n",
    "    return ind[0][0]\n",
    "def find_business_similarity(business1, business2):\n",
    "    return skipgram_model.wv.similarity(business1, business2)\n",
    "def find_user_similarity(user1, user2):\n",
    "    return skipgram_model.wv.similarity(user1, user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICF_prediction(business1, common_businesses, ratings, k_values):\n",
    "    business_ind1 = find_business(business1)\n",
    "    similar = []\n",
    "    predictions_k = []\n",
    "    \n",
    "    # For every common business find its' similarity to our business\n",
    "    for i in range(len(common_businesses)):\n",
    "        temp_ind = find_business(common_businesses[i])\n",
    "        similar.append([common_businesses[i], find_similarity(business1, common_businesses[i]), int(ratings[i])])\n",
    "    \n",
    "    # Sort similarities and take k most similar to our business\n",
    "    vectors = np.array(sorted(similar, key=itemgetter(1), reverse=True))\n",
    "    \n",
    "    # For every given k calculate the predictions for the given user\n",
    "    for k in range(len(k_values)):\n",
    "        temp_vectors = vectors[:k_values[k]]\n",
    "\n",
    "        similarity_vector = temp_vectors[:,1].astype(float)\n",
    "        ratings_vector = temp_vectors[:,2].astype(int)\n",
    "\n",
    "        # Calculate prediction\n",
    "        numerator = 0\n",
    "        denominator = np.sum(similarity_vector)\n",
    "        for i in range(len(similarity_vector)):\n",
    "            numerator += similarity_vector[i]*ratings_vector[i]\n",
    "        predictions_k.append(numerator/denominator)\n",
    "        \n",
    "    # Return a list with predictions for the user for every k value\n",
    "    return predictions_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICF predictions for all given k values in 3.693 mins\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "user_avg_pred = []\n",
    "k_values = [1,5,10,20,40,50,60,70,80,100]\n",
    "transposed_sparse = np.transpose(sparse_array)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(sample_array.shape[0]): # For every pair (u,b) that have missing rating\n",
    "    common_ratings = [] # RATINGS OF ALL BUSINESS BY USER\n",
    "    common_businesses = [] # ALL BUSINESSES THE GIVEN USER HAS RATED\n",
    "    user_index = find_index(sample_array[i][0]) # Get user index\n",
    "    common_business_indexes = np.nonzero(sparse_array[user_index,:]) # FIND BUSINESSES INDEXES FOR NONZERO VALUES ONLY!\n",
    "\n",
    "    for j in range(len(common_business_indexes[0])): # For every row in spars- (Actually for every rated business)\n",
    "        common_ratings.append(transposed_sparse[common_business_indexes[0][j]][user_index]) # Append ratings\n",
    "        common_businesses.append(unique_business[common_business_indexes[0][j]]) # Append the user_id\n",
    "    # Append the prediction\n",
    "    #user_avg_pred.append(sum(common_ratings)/len(common_ratings))\n",
    "    predictions.append(ICF_prediction(sample_array[i][1], common_businesses, common_ratings, k_values))\n",
    "\n",
    "    \n",
    "icf_predictions = np.array(predictions)\n",
    "\n",
    "stop = time.time()\n",
    "print(\"ICF predictions for all given k values in {:.3f} mins\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-0a7c1025df9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_table_R\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_table_R\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "data_table_R = sorted(data_table_R.tolist(), key=itemgetter(1,3))\n",
    "data_table_R = np.array(data_table_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPORW APLA NA KSEKINISW APO TO range(1,:) (?) ...\n",
    "j=0\n",
    "embedding = ['']\n",
    "for i in range(data_table_R.shape[0]):\n",
    "    if data_table_R[i][1] == data_table_R[i-1][1]:\n",
    "        embedding[j] += ' ' + data_table_R[i][0]\n",
    "    else:\n",
    "        embedding.append(data_table_R[i][0])\n",
    "        j+=1\n",
    "embedding = embedding[1:]\n",
    "\n",
    "for i in range(len(embedding)):\n",
    "    embedding[i] = embedding[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 100\n",
    "skipgram_model = gensim.models.Word2Vec(embedding, min_count = 1,size = embedding_size, window = 50, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCF_prediction(user1, common_users, ratings, k_values):\n",
    "    user_ind1 = find_index(user1)\n",
    "    similar = []\n",
    "    predictions_k = []\n",
    "    # For every common user find his similarity to our user\n",
    "    for i in range(len(common_users)):\n",
    "        temp_ind = find_index(common_users[i])\n",
    "        similar.append([common_users[i], find_similarity(user1, common_users[i]), int(ratings[i])])\n",
    "    \n",
    "    # Sort similarities and take k most similar to our user\n",
    "    vectors = np.array(sorted(similar, key=itemgetter(1), reverse=True))\n",
    "    \n",
    "    # For every given k calculate the predictions for the given user\n",
    "    for k in range(len(k_values)):\n",
    "        temp_vectors = vectors[:k_values[k]]\n",
    "        \n",
    "        similarity_vector = temp_vectors[:,1].astype(float)\n",
    "        ratings_vector = temp_vectors[:,2].astype(int)\n",
    "\n",
    "        # Calculate prediction\n",
    "        numerator = 0\n",
    "        denominator = np.sum(similarity_vector)\n",
    "        for i in range(len(similarity_vector)):\n",
    "            numerator += similarity_vector[i]*ratings_vector[i]\n",
    "        predictions_k.append(numerator/denominator)\n",
    "        \n",
    "    # Return a list with predictions for the user for every k value\n",
    "    return predictions_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCF predictions for all given k values in 4.743 mins\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "business_avg_pred = []\n",
    "k_values = [1,5,10,20,50,100,200,500,1000]\n",
    "\n",
    "start = time.time()\n",
    "for i in range(sample_array.shape[0]): # For every pair (u,b) that have missing rating\n",
    "    common_users = [] # USER_IDS THAT ALSO RATED THE GIVEN BUSINESS\n",
    "    common_ratings = [] # AND THEIR RATINGS\n",
    "    business_index = find_business(sample_array[i][1]) # Get business index\n",
    "    common_users_indexes = np.nonzero(sparse_array[:,business_index]) # FIND INDEXES FOR NONZERO VALUES ONLY!\n",
    "    for j in range(len(common_users_indexes[0])): # For every row in spars- (Actually for every user)\n",
    "        common_ratings.append(sparse_array[common_users_indexes[0][j]][business_index]) # Append ratings\n",
    "        common_users.append(unique_users[common_users_indexes[0][j]]) # Append the user_id\n",
    "\n",
    "    # Append the prediction\n",
    "    predictions.append(UCF_prediction(sample_array[i][0], common_users, common_ratings, k_values))\n",
    "    business_avg_pred.append(sum(common_ratings)/len(common_ratings))\n",
    "\n",
    "ucf_predictions = np.array(predictions)\n",
    "\n",
    "stop = time.time()\n",
    "print(\"UCF predictions for all given k values in {:.3f} mins\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ratings = sample_data['rating'].values.astype(float).astype(int)\n",
    "\n",
    "############################\n",
    "# RMSE FOR UCF\n",
    "RMSE_UCF = []\n",
    "for i in range(ucf_predictions[0].size):\n",
    "    predicted_ratings = np.array(ucf_predictions[:,i])\n",
    "    sum_of_difference = 0\n",
    "    for j in range(sample_data.shape[0]):\n",
    "        sum_of_difference += (predicted_ratings[j] - true_ratings[j])**2\n",
    "    RMSE_UCF.append(math.sqrt(sum_of_difference/sample_data.shape[0]))\n",
    "    \n",
    "############################\n",
    "# RMSE FOR ICF\n",
    "RMSE_ICF = []\n",
    "for i in range(icf_predictions[0].size):\n",
    "    predicted_ratings = np.array(icf_predictions[:,i])\n",
    "    sum_of_difference = 0\n",
    "    for j in range(sample_data.shape[0]):\n",
    "        sum_of_difference += (predicted_ratings[j] - true_ratings[j])**2\n",
    "    RMSE_ICF.append(math.sqrt(sum_of_difference/sample_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0259028898124114, 'RMSE_ICF']]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "results.append([min(RMSE_UCF), 'RMSE_UCF'])\n",
    "results.append([min(RMSE_ICF), 'RMSE_ICF'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
